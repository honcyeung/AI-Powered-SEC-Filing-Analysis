{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94ca9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.cloud import aiplatform \n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import firestore\n",
    "from google.cloud.aiplatform_v1.types import IndexDatapoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161b11f",
   "metadata": {},
   "source": [
    "# Get and clean SEC filing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992712f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent': 'YourName YourCompany your.email@example.com'}\n",
    "TICKER = \"SBET\"\n",
    "TARGET_FORMS = ['10-K', '10-Q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d33559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cik_from_ticker():\n",
    "\n",
    "    url = \"https://www.sec.gov/files/company_tickers.json\"\n",
    "    try:\n",
    "        response = requests.get(url, headers = HEADERS)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "        \n",
    "        for _, d in data.items():\n",
    "            if d['ticker'] == TICKER.upper():\n",
    "                cik = str(d['cik_str'])\n",
    "                cik = cik.zfill(len(cik) + 3)\n",
    "                \n",
    "        return cik\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "\n",
    "def get_sec_data(cik):\n",
    "\n",
    "    url = f\"https://data.sec.gov/submissions/CIK{cik}.json\"\n",
    "    try:\n",
    "        response = requests.get(url, headers = HEADERS)\n",
    "        response.raise_for_status() \n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "\n",
    "def parse_sec_data(data):\n",
    "\n",
    "    filing_data = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    if not filing_data:\n",
    "        raise Exception(\"No filing data found.\")\n",
    "\n",
    "    extracted_data = []\n",
    "    num_filings = len(filing_data.get(\"form\", []))\n",
    "    if num_filings:\n",
    "        for i in range(num_filings):\n",
    "            form_type = filing_data.get(\"form\", [])[i]\n",
    "            if form_type in TARGET_FORMS:\n",
    "                try:\n",
    "                    accession_number = filing_data.get('accessionNumber', [])[i]\n",
    "                    primary_document = filing_data.get('primaryDocument', [])[i]\n",
    "                    filing_date = filing_data.get('filingDate', [])[i]\n",
    "                    report_date = filing_data.get('reportDate', [])[i]\n",
    "\n",
    "                    filing_details = {\n",
    "                        'form_type': form_type,\n",
    "                        'accession_number': accession_number,\n",
    "                        'primary_document': primary_document,\n",
    "                        'filing_date': filing_date,\n",
    "                        'report_date': report_date\n",
    "                    }\n",
    "                    extracted_data.append(filing_details)\n",
    "\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Data inconsistency at index {i}. Skipping this filing.\")\n",
    "                    continue\n",
    "    return extracted_data\n",
    "\n",
    "def construct_sec_url(cik, filing_details):\n",
    "\n",
    "    url = \"https://www.sec.gov/Archives/edgar/data\"\n",
    "    target_cik = cik.lstrip(\"0\")\n",
    "    accession_number = filing_details.get(\"accession_number\", \"\").replace(\"-\", \"\")\n",
    "    primary_document = filing_details.get(\"primary_document\", \"\")\n",
    "    \n",
    "    if accession_number and primary_document:\n",
    "        url = f\"{url}/{target_cik}/{accession_number}/{primary_document}\"\n",
    "    else:\n",
    "        raise Exception(\"Info is missing.\")\n",
    "\n",
    "    return url\n",
    "\n",
    "def get_html_text(url):\n",
    "\n",
    "    try:\n",
    "        res = requests.get(url, headers = HEADERS)\n",
    "        res.raise_for_status()\n",
    "        html_text = res.text\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching data: {e}\")\n",
    "\n",
    "    return html_text\n",
    "\n",
    "def html_table_to_markdown(table_tag):\n",
    "    \"\"\"\n",
    "    Converts a BeautifulSoup table tag into a Markdown formatted string.\n",
    "    This helps preserve the structure of financial data for the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    # process table headers\n",
    "    headers = [th.get_text(strip = True).replace(\"\\n\", \"\") for th in table_tag.find_all(\"th\")]\n",
    "\n",
    "    # calculate the space between cells?\n",
    "    if headers:\n",
    "        markdown_lines.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "        markdown_lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
    "\n",
    "    # process table rows\n",
    "    for row in table_tag.find_all(\"tr\"):\n",
    "        cells = [td.get_text(strip = True).replace(\"\\n\", \"\") for td in row.find_all([\"td\", \"th\"])]\n",
    "        # only add rows that have content and match the header count if headers exist\n",
    "        if cells and (not headers or len(cells) == len(headers)):\n",
    "            markdown_lines.append(\"| \" + \" | \".join(cells) + \" |\")\n",
    "\n",
    "    return \"\\n\\n\" + \"\\n\".join(markdown_lines) + \"\\n\\n\" \n",
    "\n",
    "def clean_html(html_text):\n",
    "\n",
    "    if not html_text:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "\n",
    "    # decompose (completely remove) all script, style, and other non-content tags\n",
    "    for tag in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
    "        tag.decompose()\n",
    "\n",
    "    # convert tables to Markdown and replace the original table tag\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        markdown_text_tag = soup.new_string(html_table_to_markdown(table))\n",
    "        table.replace_with(markdown_text_tag)\n",
    "\n",
    "    text = soup.get_text(separator = \"\\n\", strip = True)\n",
    "    # remove excessive blank lines to make it more readable\n",
    "    cleaned_text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def chunk_filing_by_section(cleaned_text, metadata):\n",
    "\n",
    "    pattern = r'(?i)(item\\s*\\d+[a-z]?\\.?)'\n",
    "    parts = re.split(pattern, cleaned_text)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    intro_content = parts[0].strip()\n",
    "    if len(intro_content.split()) > 20:\n",
    "        chunks.append({\n",
    "            \"content\": intro_content,\n",
    "            \"metadata\": {**metadata, \"section\": \"Introduction\"}\n",
    "        })\n",
    "\n",
    "    # The rest of the list is ['Item 1.', 'Content of Item 1...', 'Item 1A.', 'Content of 1A...']\n",
    "    # We iterate through them in pairs.\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i].strip()\n",
    "        content = parts[i + 1].strip() if (i + 1) < len(parts) else \"\"\n",
    "\n",
    "        chunk_content = f\"{header}\\n\\n{content}\"\n",
    "\n",
    "        if len(content.split()) > 20:\n",
    "            chunk_obj = {\n",
    "                \"content\": chunk_content,\n",
    "                \"metadata\": {**metadata, \"section\": header}\n",
    "            }\n",
    "            chunks.append(chunk_obj)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e2f859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ching/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "cik = get_cik_from_ticker()\n",
    "data = get_sec_data(cik)\n",
    "extracted_data = parse_sec_data(data)\n",
    "\n",
    "# test 1 filing first\n",
    "if extracted_data:\n",
    "    for filing in extracted_data:\n",
    "        url = construct_sec_url(cik, filing)\n",
    "        filing[\"url\"] = url\n",
    "        html_text = get_html_text(url)\n",
    "        cleaned_text = clean_html(html_text)\n",
    "        chunks = chunk_filing_by_section(cleaned_text, filing)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ebb20",
   "metadata": {},
   "source": [
    "# Google Cloud and Gemini configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "REGION = \"europe-west2\"\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "GEMINI_EMBEDDING_MODEL = \"models/embedding-001\"\n",
    "GEMINI_EMBEDDING_MODEL_DIMENSION = 768 # Gemini embedding-001 model has 768 dimensions\n",
    "COLLECTION_NAME = os.environ[\"COLLECTION_NAME\"]\n",
    "INDEX_NAME = os.environ[\"INDEX_NAME\"]\n",
    "INDEX_ENDPOINT_NAME = os.environ[\"INDEX_ENDPOINT_NAME\"]\n",
    "\n",
    "client = genai.Client(api_key = GEMINI_API_KEY)\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b172539",
   "metadata": {},
   "source": [
    "### Retrieval part of Retrieval-Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff75776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunk_in_firestore(chunk_id, chunk_data):\n",
    "\n",
    "    try:\n",
    "        db = firestore.Client(PROJECT_ID)\n",
    "        doc_ref = db.collection(COLLECTION_NAME).document(chunk_id)\n",
    "        if doc_ref.get().exists:\n",
    "            print(f\"Document with ID: {chunk_id} already exists. Skipping write.\")\n",
    "            return\n",
    "        doc_ref.set(chunk_data)\n",
    "        print(f\"Successfully stored document with ID: {chunk_id} in collection '{collection_name}'\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing document {chunk_id}: {e}\")\n",
    "\n",
    "def get_chunk_in_firestore(chunk_id):\n",
    "\n",
    "    try:\n",
    "        db = firestore.Client(PROJECT_ID)\n",
    "        doc_ref = db.collection(COLLECTION_NAME).document(chunk_id)\n",
    "        doc = doc_ref.get()\n",
    "\n",
    "        if doc.exists:\n",
    "            print(f\"Successfully retrieved document: {chunk_id}\")\n",
    "            \n",
    "            return doc.to_dict()\n",
    "        else:\n",
    "            print(f\"No document found with ID: {chunk_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving document {chunk_id}: {e}\")\n",
    "\n",
    "def create_and_deploy_vector_index(chunks_for_index):\n",
    "    \"\"\"\n",
    "    Creates a Vertex AI Vector Search Index, an Index Endpoint, and deploys the index.\n",
    "    This is a one-time setup process.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the index\n",
    "    try:\n",
    "        vector_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "            display_name = INDEX_NAME,\n",
    "            dimensions = GEMINI_EMBEDDING_MODEL_DIMENSION,\n",
    "            approximate_neighbors_count = 150,\n",
    "            distance_measure_type = \"DOT_PRODUCT_DISTANCE\",\n",
    "            index_update_method = \"STREAM_UPDATE\"\n",
    "        )\n",
    "        index_id = vector_index.resource_name.split(\"/\")[-1]\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"Index '{INDEX_NAME}' already exists. Reusing it.\")\n",
    "        else:\n",
    "            raise Exception(e)\n",
    "    \n",
    "    # Create an Index Endpoint\n",
    "    try:\n",
    "        index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "            display_name = INDEX_ENDPOINT_NAME, public_endpoint_enabled = True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"Endpoint '{INDEX_ENDPOINT_NAME}' already exists. Reusing it.\")\n",
    "            index_endpoint = aiplatform.MatchingEngineIndexEndpoint.list(filter = f'display_name=\"{INDEX_ENDPOINT_NAME}\"')[0]\n",
    "        else:\n",
    "            raise Exception(e)\n",
    "\n",
    "    # Deploy the Index to the Endpoint\n",
    "    try:\n",
    "        # A unique ID for this deployment\n",
    "        deployed_index_id = f\"gemini_deployed_{int(time.time())}\" \n",
    "        index_endpoint.deploy_index(\n",
    "            index = vector_index, deployed_index_id = deployed_index_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"has been deployed\" in str(e):\n",
    "            print(\"Index is already deployed to this endpoint.\")\n",
    "        else:\n",
    "            raise Exception(e)\n",
    "\n",
    "    return index_id\n",
    "\n",
    "def generate_embeddings_and_prepare_datapoints(chunks):\n",
    "    \"\"\"\n",
    "    Takes a list of chunk dictionaries, generates an embedding for each using Gemini,\n",
    "    and formats them for uploading to Vertex AI Vector Search.\n",
    "    \"\"\"\n",
    "\n",
    "    datapoints = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            response = client.models.embed_content(\n",
    "                model = GEMINI_EMBEDDING_MODEL,\n",
    "                contents = chunk[\"content\"],\n",
    "                config = types.EmbedContentConfig(task_type = \"RETRIEVAL_DOCUMENT\"),\n",
    "            ).embeddings\n",
    "            embedding_vector = response[0].values\n",
    "\n",
    "            # Create the datapoint structure required by Vertex AI\n",
    "            # store unique id and embedding only\n",
    "            datapoint = IndexDatapoint(\n",
    "                datapoint_id = f\"{TICKER}-{chunk['metadata']['form_type']}-{chunk['metadata']['accession_number']}-{i}\",\n",
    "                feature_vector = embedding_vector\n",
    "            )\n",
    "            datapoints.append(datapoint)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for chunk {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return datapoints\n",
    "\n",
    "def upload_datapoints_to_vertex_ai(index_resource_name, datapoints):\n",
    "\n",
    "    try:\n",
    "        index = aiplatform.MatchingEngineIndex(index_name = index_resource_name)\n",
    "        index.upsert_datapoints(datapoints = datapoints)\n",
    "    except Exception as e:\n",
    "        raise Exception(e)\n",
    "\n",
    "def get_index_id():\n",
    "\n",
    "    try:\n",
    "        indexes = aiplatform.MatchingEngineIndex.list(\n",
    "            filter = f'display_name=\"{INDEX_NAME}\"'\n",
    "        )\n",
    "        index = indexes[0].resource_name.split(\"/\")[-1]\n",
    "        \n",
    "        return index\n",
    "\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c10daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store metadata on Firestore\n",
    "for i, chunk in enumerate(chunks):\n",
    "    id = f\"{TICKER}-{chunk['metadata']['form_type']}-{chunk['metadata']['accession_number']}-{i}\"\n",
    "    store_chunk_in_firestore(id, chunk)\n",
    "\n",
    "# get metadata of the chunks\n",
    "# doc = get_chunk_in_firestore(\"chunk_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc901a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-time setup\n",
    "# index_id = create_and_deploy_vector_index(INDEX_NAME, INDEX_ENDPOINT_NAME, chunks)\n",
    "\n",
    "index_id = get_index_id()\n",
    "datapoints = generate_embeddings_and_prepare_datapoints(chunks)\n",
    "\n",
    "if datapoints:\n",
    "    index_resource_name = f\"projects/{PROJECT_ID}/locations/{REGION}/indexes/{index_id}\" \n",
    "    upload_datapoints_to_vertex_ai(index_resource_name, datapoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffaeeeb",
   "metadata": {},
   "source": [
    "### Augmented Generation part of RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "57776c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "GEMINI_GENERATIVE_MODEL = \"gemini-2.5-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7506b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relevant_chunks(query, index_id, num_results = 5):\n",
    "    \"\"\"\n",
    "    Takes a user's query, generates an embedding, and finds the most similar chunks in the Vertex AI Vector Search index.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.models.embed_content(\n",
    "            model = GEMINI_EMBEDDING_MODEL,\n",
    "            contents = query,\n",
    "            config = types.EmbedContentConfig(task_type = \"RETRIEVAL_QUERY\"),\n",
    "        ).embeddings\n",
    "        embedding_vector = response[0].values\n",
    "    except Exception as e:\n",
    "        print(f\"Can't generate embeddings for the query: {e}\") \n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        index_endpoint = aiplatform.MatchingEngineIndexEndpoint(index_endpoint_name = INDEX_ENDPOINT_NAME)\n",
    "        response = index_endpoint.find_neighbors(\n",
    "            deployed_index_id = index_id,\n",
    "            queries = embedding_vector,\n",
    "            num_neighbors = num_results\n",
    "        )\n",
    "        matched_ids = [neighbor.id for neighbor in response[0]]\n",
    "        print(f\"Found {len(matched_ids)} matching document IDs.\")\n",
    "\n",
    "        return matched_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error querying Vertex AI: {e}\")\n",
    "        return\n",
    "\n",
    "def retrieve_chunks_from_firestore(chunk_ids):\n",
    "    \"\"\"\n",
    "    Retrieves the full text and metadata for a list of chunk IDs from Firestore.\n",
    "    \"\"\"\n",
    "\n",
    "    retrieved_chunks = []\n",
    "    for chunk_id in chunk_ids:\n",
    "        doc = get_chunk_in_firestore(chunk_id)\n",
    "        retrieved_chunks.append(doc)\n",
    "\n",
    "    return retrieved_chunks\n",
    "\n",
    "def augemnt_and_generate_answer(query, context_chunks):\n",
    "    \"\"\"\n",
    "    Combines the user's query and the retrieved context into a prompt, then asks the AI model to generate a final answer.\n",
    "    \"\"\"\n",
    "\n",
    "    context_str = \"\\n---\\n\".join([chunk['content'] for chunk in context_chunks])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful financial analyst assistant. Answer the following question based ONLY on the context provided below.\n",
    "    If the context does not contain the answer, say \"I cannot answer this question based on the provided context.\"\n",
    "\n",
    "    QUESTION:\n",
    "    {query}\n",
    "\n",
    "    CONTEXT:\n",
    "    {context_str}\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = client.models.generate_content(\n",
    "            model = GEMINI_GENERATIVE_MODEL,\n",
    "            contents = prompt\n",
    "        )\n",
    "        return response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer with Gemini: {e}\")\n",
    "        return\n",
    "\n",
    "def ask_question(query):\n",
    "    \"\"\"\n",
    "    The main function that orchestrates the entire RAG pipeline.\n",
    "    \"\"\"\n",
    "\n",
    "    relavant_ids = find_relevant_chunks(query, index_id)\n",
    "    if not relavant_ids:\n",
    "        print(\"Could not find any relevant documents.\")\n",
    "        return\n",
    "    \n",
    "    context_data = retrieve_chunks_from_firestore(relevant_ids)\n",
    "    if not context_data:\n",
    "        print(\"Could not retrieve document content from Firestore.\")\n",
    "        return\n",
    "\n",
    "    final_ans = augemnt_and_generate_answer(query, context_data)\n",
    "    print(\"\\n--- FINAL ANSWER ---\")\n",
    "    print(final_answer)\n",
    "    print(\"\\n--- SOURCES ---\")\n",
    "    for chunk in context_data:\n",
    "        print(f\"- {chunk['metadata']['form_type']}, Report Date: {chunk['metadata']['report_date']}, Section: {chunk['metadata']['section']}, URL: {chunk['metadata']['url']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3355c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"\"\n",
    "    \n",
    "ask_question(user_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
