{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "94ca9888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from google.cloud import aiplatform \n",
    "from dotenv import load_dotenv\n",
    "from google.cloud import firestore\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message = \"\"\"Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
    "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1161b11f",
   "metadata": {},
   "source": [
    "# get and clean sec filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7401e99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {'User-Agent': 'YourName YourCompany your.email@example.com'}\n",
    "\n",
    "TICKER = \"SBET\"\n",
    "CIK = \"0001981535\"\n",
    "url = f\"https://data.sec.gov/submissions/CIK{CIK}.json\"\n",
    "response = requests.get(url, headers = HEADERS)\n",
    "response.raise_for_status() \n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d33559a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sec_data(data, target_forms = ['10-K', '10-Q']):\n",
    "\n",
    "    filing_data = data.get(\"filings\", {}).get(\"recent\", {})\n",
    "    if not filing_data:\n",
    "        raise Exception(\"No filing data found.\")\n",
    "\n",
    "    extracted_data = []\n",
    "    num_filings = len(filing_data.get(\"form\", []))\n",
    "    if num_filings:\n",
    "        for i in range(num_filings):\n",
    "            form_type = filing_data.get(\"form\", [])[i]\n",
    "            if form_type in target_forms:\n",
    "                try:\n",
    "                    accession_number = filing_data.get('accessionNumber', [])[i]\n",
    "                    primary_document = filing_data.get('primaryDocument', [])[i]\n",
    "                    filing_date = filing_data.get('filingDate', [])[i]\n",
    "                    report_date = filing_data.get('reportDate', [])[i]\n",
    "\n",
    "                    filing_details = {\n",
    "                        'form_type': form_type,\n",
    "                        'accession_number': accession_number,\n",
    "                        'primary_document': primary_document,\n",
    "                        'filing_date': filing_date,\n",
    "                        'report_date': report_date\n",
    "                    }\n",
    "                    extracted_data.append(filing_details)\n",
    "\n",
    "                except IndexError:\n",
    "                    print(f\"Warning: Data inconsistency at index {i}. Skipping this filing.\")\n",
    "                    continue\n",
    "    return extracted_data\n",
    "\n",
    "def construct_sec_url(filing_details):\n",
    "\n",
    "    DOMAIN = \"https://www.sec.gov/Archives/edgar/data\"\n",
    "    target_cik = CIK.lstrip(\"0\")\n",
    "    accession_number = filing_details.get(\"accession_number\", \"\").replace(\"-\", \"\")\n",
    "    primary_document = filing_details.get(\"primary_document\", \"\")\n",
    "    \n",
    "    if accession_number and primary_document:\n",
    "        url = f\"{DOMAIN}/{target_cik}/{accession_number}/{primary_document}\"\n",
    "    else:\n",
    "        raise Exception(\"Info is missing.\")\n",
    "\n",
    "    return url\n",
    "\n",
    "def get_html_text(url):\n",
    "\n",
    "    res = requests.get(url, headers = HEADERS)\n",
    "    res.raise_for_status()\n",
    "    html_text = res.text\n",
    "\n",
    "    return html_text\n",
    "\n",
    "def html_table_to_markdown(table_tag):\n",
    "    \"\"\"\n",
    "    Converts a BeautifulSoup table tag into a Markdown formatted string.\n",
    "    This helps preserve the structure of financial data for the LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    markdown_lines = []\n",
    "    # process table headers\n",
    "    headers = [th.get_text(strip = True).replace(\"\\n\", \"\") for th in table_tag.find_all(\"th\")]\n",
    "\n",
    "    # calculate the space between cells?\n",
    "    if headers:\n",
    "        markdown_lines.append(\"| \" + \" | \".join(headers) + \" |\")\n",
    "        markdown_lines.append(\"| \" + \" | \".join([\"---\"] * len(headers)) + \" |\")\n",
    "\n",
    "    # process table rows\n",
    "    for row in table_tag.find_all(\"tr\"):\n",
    "        cells = [td.get_text(strip = True).replace(\"\\n\", \"\") for td in row.find_all([\"td\", \"th\"])]\n",
    "        # only add rows that have content and match the header count if headers exist\n",
    "        if cells and (not headers or len(cells) == len(headers)):\n",
    "            markdown_lines.append(\"| \" + \" | \".join(cells) + \" |\")\n",
    "\n",
    "    return \"\\n\\n\" + \"\\n\".join(markdown_lines) + \"\\n\\n\" \n",
    "\n",
    "def clean_html(html_text):\n",
    "\n",
    "    if not html_text:\n",
    "        return \"\"\n",
    "\n",
    "    soup = BeautifulSoup(html_text, \"lxml\")\n",
    "\n",
    "    # decompose (completely remove) all script, style, and other non-content tags\n",
    "    for tag in soup(['script', 'style', 'header', 'footer', 'nav']):\n",
    "        tag.decompose()\n",
    "\n",
    "    # convert tables to Markdown and replace the original table tag\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        markdown_text_tag = soup.new_string(html_table_to_markdown(table))\n",
    "        table.replace_with(markdown_text_tag)\n",
    "\n",
    "    text = soup.get_text(separator = \"\\n\", strip = True)\n",
    "    # remove excessive blank lines to make it more readable\n",
    "    cleaned_text = re.sub(r'\\n\\n+', '\\n\\n', text)\n",
    "\n",
    "    return cleaned_text\n",
    "\n",
    "def chunk_filing_by_section(cleaned_text, metadata):\n",
    "\n",
    "    pattern = r'(?i)(item\\s*\\d+[a-z]?\\.?)'\n",
    "    parts = re.split(pattern, cleaned_text)\n",
    "\n",
    "    chunks = []\n",
    "\n",
    "    intro_content = parts[0].strip()\n",
    "    if len(intro_content.split()) > 20:\n",
    "        chunks.append({\n",
    "            \"content\": intro_content,\n",
    "            \"metadata\": {**metadata, \"section\": \"Introduction\"}\n",
    "        })\n",
    "\n",
    "    # The rest of the list is ['Item 1.', 'Content of Item 1...', 'Item 1A.', 'Content of 1A...']\n",
    "    # We iterate through them in pairs.\n",
    "    for i in range(1, len(parts), 2):\n",
    "        header = parts[i].strip()\n",
    "        content = parts[i + 1].strip() if (i + 1) < len(parts) else \"\"\n",
    "\n",
    "        chunk_content = f\"{header}\\n\\n{content}\"\n",
    "\n",
    "        if len(content.split()) > 20:\n",
    "            chunk_obj = {\n",
    "                \"content\": chunk_content,\n",
    "                \"metadata\": {**metadata, \"section\": header}\n",
    "            }\n",
    "            chunks.append(chunk_obj)\n",
    "\n",
    "    return chunks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "11e2f859",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ching/opt/anaconda3/lib/python3.9/site-packages/bs4/builder/__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "extracted_data = parse_sec_data(data)\n",
    "\n",
    "# test 1 filing first\n",
    "if extracted_data:\n",
    "    for filing in extracted_data:\n",
    "        url = construct_sec_url(filing)\n",
    "        filing[\"url\"] = url\n",
    "        html_text = get_html_text(url)\n",
    "        cleaned_text = clean_html(html_text)\n",
    "        chunks = chunk_filing_by_section(cleaned_text, filing)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235ebb20",
   "metadata": {},
   "source": [
    "# google cloud and gemini configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2f4d2dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "PROJECT_ID = os.environ[\"PROJECT_ID\"]\n",
    "REGION = \"europe-west2\"\n",
    "GEMINI_API_KEY = os.environ[\"GEMINI_API_KEY\"]\n",
    "GEMINI_EMBEDDING_MODEL = \"models/embedding-001\"\n",
    "COLLECTION_NAME = \"sec-filing\"\n",
    "INDEX_NAME = os.environ[\"INDEX_NAME\"]\n",
    "INDEX_ENDPOINT_NAME = os.environ[\"INDEX_ENDPOINT_NAME\"]\n",
    "\n",
    "client = genai.Client(api_key = GEMINI_API_KEY)\n",
    "aiplatform.init(project = PROJECT_ID, location = REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed85003",
   "metadata": {},
   "source": [
    "\"https://ai.google.dev/gemini-api/docs/text-generation\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model = \"gemini-2.5-flash\",\n",
    "    contents = \"Explain how AI works in a few words\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "fff75776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_chunk_in_firestore(chunk_id, chunk_data, project_id = PROJECT_ID, collection_name = COLLECTION_NAME):\n",
    "\n",
    "    try:\n",
    "        db = firestore.Client(project_id)\n",
    "        doc_ref = db.collection(collection_name).document(chunk_id)\n",
    "        doc_ref.set(chunk_data)\n",
    "\n",
    "        print(f\"Successfully stored document with ID: {chunk_id} in collection '{collection_name}'\")\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error storing document {chunk_id}: {e}\")\n",
    "\n",
    "        return False\n",
    "\n",
    "def create_and_deploy_vector_index(index_name, endpoint_name, chunks_for_index):\n",
    "    \"\"\"\n",
    "    Creates a Vertex AI Vector Search Index, an Index Endpoint, and deploys the index.\n",
    "    This is a one-time setup process.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create the index\n",
    "    try:\n",
    "        # Gemini embedding-001 model has 768 dimensions\n",
    "        vector_index = aiplatform.MatchingEngineIndex.create_tree_ah_index(\n",
    "            display_name = index_name,\n",
    "            dimensions = 768,\n",
    "            approximate_neighbors_count = 150,\n",
    "            distance_measure_type = \"DOT_PRODUCT_DISTANCE\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"Index '{index_name}' already exists. Reusing it.\")\n",
    "        else:\n",
    "            raise Exception(e)\n",
    "    \n",
    "    # Create an Index Endpoint\n",
    "    try:\n",
    "        index_endpoint = aiplatform.MatchingEngineIndexEndpoint.create(\n",
    "            display_name = endpoint_name, public_endpoint_enabled = True\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"already exists\" in str(e):\n",
    "            print(f\"Endpoint '{endpoint_name}' already exists. Reusing it.\")\n",
    "            index_endpoint = aiplatform.MatchingEngineIndexEndpoint.list(filter = f'display_name=\"{endpoint_name}\"')[0]\n",
    "        else:\n",
    "            raise Exception(e)\n",
    "\n",
    "    # Deploy the Index to the Endpoint\n",
    "    try:\n",
    "        # A unique ID for this deployment\n",
    "        deployed_index_id = f\"gemini_deployed_{int(time.time())}\" \n",
    "        index_endpoint.deploy_index(\n",
    "            index = vector_index, deployed_index_id = deployed_index_id\n",
    "        )\n",
    "    except Exception as e:\n",
    "        if \"has been deployed\" in str(e):\n",
    "            print(\"Index is already deployed to this endpoint.\")\n",
    "        else:\n",
    "            raise Exception(e)\n",
    "\n",
    "    return vector_index, index_endpoint\n",
    "\n",
    "def generate_embeddings_and_prepare_datapoints(chunks):\n",
    "    \"\"\"\n",
    "    Takes a list of chunk dictionaries, generates an embedding for each using Gemini,\n",
    "    and formats them for uploading to Vertex AI Vector Search.\n",
    "    \"\"\"\n",
    "\n",
    "    datapoints = []\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        try:\n",
    "            response = client.models.embed_content(\n",
    "                model = GEMINI_EMBEDDING_MODEL,\n",
    "                contents = chunk[\"content\"],\n",
    "                config = types.EmbedContentConfig(task_type = \"RETRIEVAL_DOCUMENT\"),\n",
    "                # title = f\"SEC Filing Chunk: {chunk['metadata']['section']}\"\n",
    "            ).embeddings\n",
    "            embedding_vector = response[\"embedding\"]\n",
    "\n",
    "            # Create the datapoint structure required by Vertex AI\n",
    "            # store unique id and embedding only\n",
    "            datapoint = {\n",
    "                \"id\": f\"{TICKER}-{chunk['metadata']['form_type']}-{chunk['metadata']['accession_number']}-{i}\",\n",
    "                \"embedding\": embedding_vector\n",
    "            }\n",
    "            datapoints.append(datapoint)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating embedding for chunk {i}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return datapoints\n",
    "\n",
    "def upload_datapoints_to_vertex_ai(index_resource_name, datapoints):\n",
    "\n",
    "    try:\n",
    "        index = aiplatform.MatchingEngineIndex(index_name = index_resource_name)\n",
    "        index.upsert_datapoints(datapoints = datapoints)\n",
    "    except Exception as e:\n",
    "        raise Exception(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c10daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    id = f\"{TICKER}-{chunk['metadata']['form_type']}-{chunk['metadata']['accession_number']}-{i}\"\n",
    "    store_chunk_in_firestore(chunk_id = id, chunk_data = chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc901a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating embedding for chunk 0: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 1: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 2: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 3: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 4: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 5: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 6: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 7: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 8: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 9: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 10: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 11: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 12: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 13: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 14: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 15: list indices must be integers or slices, not str\n",
      "Error generating embedding for chunk 16: list indices must be integers or slices, not str\n"
     ]
    }
   ],
   "source": [
    "# one-time setup\n",
    "# create_and_deploy_vector_index(INDEX_NAME, INDEX_ENDPOINT_NAME, chunks)\n",
    "\n",
    "datapoints = generate_embeddings_and_prepare_datapoints(chunks)\n",
    "\n",
    "# if datapoints:\n",
    "#     index_resource_name = f\"projects/{PROJECT_ID}/locations/{REGION}/indexes/{INDEX_NAME}\" \n",
    "#     upload_datapoints_to_vertex(index_resource_name, datapoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "cb75cebf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for i, chunk in enumerate(chunks):\n",
    "    print(type(chunk['content']))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9000d088",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
